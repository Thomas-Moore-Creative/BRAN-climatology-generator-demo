{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c7829f-9399-4bfb-adf4-63851ba5fba2",
   "metadata": {},
   "source": [
    "# 3D stats sanbox for the composites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85e17d-1666-4bdd-bbb2-ba7288e8e71a",
   "metadata": {},
   "source": [
    "Date: 18 May, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b722e90-e1fa-45a1-9638-5d9732063024",
   "metadata": {},
   "source": [
    "Author = {\"name\": \"Thomas Moore\", \"affiliation\": \"CSIRO\", \"email\": \"thomas.moore@csiro.au\", \"orcid\": \"0000-0003-3930-1946\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23449033-f7fe-4f01-ac2d-de773de8db6b",
   "metadata": {},
   "source": [
    "### BRAN2020 is over 50TB of `float32` data over nearly 9000 `netcdf` file assests in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41a28d-7966-420a-a2ba-5bb53ada1d9c",
   "metadata": {},
   "source": [
    "#### required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfd2435-2231-4a62-aa70-d5fd531a259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.distributed\n",
    "import dask\n",
    "import datetime\n",
    "import zarr\n",
    "\n",
    "from rechunker import rechunk\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "import subprocess\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2872f4-33f3-4ef6-8daa-6e743eafc6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x14b38006f430>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set configuration options\n",
    "dask.config.set({\n",
    "    'distributed.comm.timeouts.connect': '90s',  # Timeout for connecting to a worker\n",
    "    'distributed.comm.timeouts.tcp': '90s',  # Timeout for TCP communications\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59cd9b1a-78b4-4adb-96b1-40726aa96288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distributed': {'dashboard': {'link': '/proxy/{port}/status',\n",
       "   'export-tool': False,\n",
       "   'graph-max-items': 5000,\n",
       "   'prometheus': {'namespace': 'dask'}},\n",
       "  'version': 2,\n",
       "  'scheduler': {'allowed-failures': 3,\n",
       "   'bandwidth': 100000000,\n",
       "   'blocked-handlers': [],\n",
       "   'contact-address': None,\n",
       "   'default-data-size': '1kiB',\n",
       "   'events-cleanup-delay': '1h',\n",
       "   'idle-timeout': None,\n",
       "   'no-workers-timeout': None,\n",
       "   'work-stealing': True,\n",
       "   'work-stealing-interval': '100ms',\n",
       "   'worker-saturation': 1.1,\n",
       "   'worker-ttl': '5 minutes',\n",
       "   'preload': [],\n",
       "   'preload-argv': [],\n",
       "   'unknown-task-duration': '500ms',\n",
       "   'default-task-durations': {'rechunk-split': '1us',\n",
       "    'split-shuffle': '1us',\n",
       "    'split-taskshuffle': '1us',\n",
       "    'split-stage': '1us'},\n",
       "   'validate': False,\n",
       "   'dashboard': {'status': {'task-stream-length': 1000},\n",
       "    'tasks': {'task-stream-length': 100000},\n",
       "    'tls': {'ca-file': None, 'key': None, 'cert': None},\n",
       "    'bokeh-application': {'allow_websocket_origin': ['*'],\n",
       "     'keep_alive_milliseconds': 500,\n",
       "     'check_unused_sessions_milliseconds': 500}},\n",
       "   'locks': {'lease-validation-interval': '10s', 'lease-timeout': '30s'},\n",
       "   'http': {'routes': ['distributed.http.scheduler.prometheus',\n",
       "     'distributed.http.scheduler.info',\n",
       "     'distributed.http.scheduler.json',\n",
       "     'distributed.http.health',\n",
       "     'distributed.http.proxy',\n",
       "     'distributed.http.statics']},\n",
       "   'allowed-imports': ['dask', 'distributed'],\n",
       "   'active-memory-manager': {'start': True,\n",
       "    'interval': '2s',\n",
       "    'measure': 'optimistic',\n",
       "    'policies': [{'class': 'distributed.active_memory_manager.ReduceReplicas'}]}},\n",
       "  'worker': {'blocked-handlers': [],\n",
       "   'multiprocessing-method': 'spawn',\n",
       "   'use-file-locking': True,\n",
       "   'transfer': {'message-bytes-limit': '50MB'},\n",
       "   'connections': {'outgoing': 50, 'incoming': 10},\n",
       "   'preload': [],\n",
       "   'preload-argv': [],\n",
       "   'daemon': True,\n",
       "   'validate': False,\n",
       "   'resources': {},\n",
       "   'lifetime': {'duration': None, 'stagger': '0 seconds', 'restart': False},\n",
       "   'profile': {'enabled': True,\n",
       "    'interval': '10ms',\n",
       "    'cycle': '1000ms',\n",
       "    'low-level': False},\n",
       "   'memory': {'recent-to-old-time': '30s',\n",
       "    'rebalance': {'measure': 'optimistic',\n",
       "     'sender-min': 0.3,\n",
       "     'recipient-max': 0.6,\n",
       "     'sender-recipient-gap': 0.1},\n",
       "    'transfer': 0.1,\n",
       "    'target': 0.6,\n",
       "    'spill': 0.7,\n",
       "    'pause': 0.8,\n",
       "    'terminate': 0.95,\n",
       "    'max-spill': False,\n",
       "    'spill-compression': 'auto',\n",
       "    'monitor-interval': '100ms'},\n",
       "   'http': {'routes': ['distributed.http.worker.prometheus',\n",
       "     'distributed.http.health',\n",
       "     'distributed.http.statics']}},\n",
       "  'nanny': {'preload': [],\n",
       "   'preload-argv': [],\n",
       "   'environ': {},\n",
       "   'pre-spawn-environ': {'MALLOC_TRIM_THRESHOLD_': 65536,\n",
       "    'OMP_NUM_THREADS': 1,\n",
       "    'MKL_NUM_THREADS': 1,\n",
       "    'OPENBLAS_NUM_THREADS': 1}},\n",
       "  'client': {'heartbeat': '5s',\n",
       "   'scheduler-info-interval': '2s',\n",
       "   'security-loader': None,\n",
       "   'preload': [],\n",
       "   'preload-argv': []},\n",
       "  'deploy': {'lost-worker-timeout': '15s', 'cluster-repr-interval': '500ms'},\n",
       "  'adaptive': {'interval': '1s',\n",
       "   'target-duration': '5s',\n",
       "   'minimum': 0,\n",
       "   'maximum': inf,\n",
       "   'wait-count': 3},\n",
       "  'comm': {'retry': {'count': 0, 'delay': {'min': '1s', 'max': '20s'}},\n",
       "   'compression': False,\n",
       "   'shard': '64MiB',\n",
       "   'offload': '10MiB',\n",
       "   'default-scheme': 'tcp',\n",
       "   'socket-backlog': 2048,\n",
       "   'ucx': {'cuda-copy': None,\n",
       "    'tcp': None,\n",
       "    'nvlink': None,\n",
       "    'infiniband': None,\n",
       "    'rdmacm': None,\n",
       "    'create-cuda-context': None,\n",
       "    'environment': {}},\n",
       "   'zstd': {'level': 3, 'threads': 0},\n",
       "   'timeouts': {'connect': '90s', 'tcp': '90s'},\n",
       "   'require-encryption': None,\n",
       "   'tls': {'ciphers': None,\n",
       "    'min-version': 1.2,\n",
       "    'max-version': None,\n",
       "    'ca-file': None,\n",
       "    'scheduler': {'cert': None, 'key': None},\n",
       "    'worker': {'key': None, 'cert': None},\n",
       "    'client': {'key': None, 'cert': None}},\n",
       "   'websockets': {'shard': '8MiB'}},\n",
       "  'diagnostics': {'nvml': True,\n",
       "   'cudf': False,\n",
       "   'computations': {'max-history': 100,\n",
       "    'nframes': 0,\n",
       "    'ignore-modules': ['asyncio',\n",
       "     'functools',\n",
       "     'threading',\n",
       "     'datashader',\n",
       "     'dask',\n",
       "     'debugpy',\n",
       "     'distributed',\n",
       "     'coiled',\n",
       "     'cudf',\n",
       "     'cuml',\n",
       "     'matplotlib',\n",
       "     'pluggy',\n",
       "     'prefect',\n",
       "     'rechunker',\n",
       "     'xarray',\n",
       "     'xgboost',\n",
       "     'xdist'],\n",
       "    'ignore-files': ['runpy\\\\.py',\n",
       "     'pytest',\n",
       "     'py\\\\.test',\n",
       "     'pytest-script\\\\.py',\n",
       "     '_pytest',\n",
       "     'pycharm',\n",
       "     'vscode_pytest',\n",
       "     'get_output_via_markers\\\\.py']},\n",
       "   'erred-tasks': {'max-history': 100}},\n",
       "  'p2p': {'comm': {'retry': {'count': 10,\n",
       "     'delay': {'min': '1s', 'max': '30s'}}},\n",
       "   'disk': True},\n",
       "  'admin': {'large-graph-warning-threshold': '10MB',\n",
       "   'tick': {'interval': '20ms', 'limit': '3s', 'cycle': '1s'},\n",
       "   'max-error-length': 10000,\n",
       "   'log-length': 10000,\n",
       "   'log-format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
       "   'low-level-log-length': 1000,\n",
       "   'pdb-on-err': False,\n",
       "   'system-monitor': {'interval': '500ms',\n",
       "    'log-length': 7200,\n",
       "    'disk': True,\n",
       "    'host-cpu': False,\n",
       "    'gil': {'enabled': True, 'interval': '1ms'}},\n",
       "   'event-loop': 'tornado'},\n",
       "  'rmm': {'pool-size': None}},\n",
       " 'temporary_directory': '/jobfs/115971413.gadi-pbs',\n",
       " 'visualization': {'engine': None},\n",
       " 'tokenize': {'ensure-deterministic': False},\n",
       " 'dataframe': {'backend': 'pandas',\n",
       "  'shuffle': {'method': None, 'compression': None},\n",
       "  'parquet': {'metadata-task-size-local': 512, 'metadata-task-size-remote': 1},\n",
       "  'convert-string': None,\n",
       "  'query-planning': None},\n",
       " 'array': {'backend': 'numpy',\n",
       "  'chunk-size': '128MiB',\n",
       "  'rechunk': {'method': 'tasks', 'threshold': 4},\n",
       "  'svg': {'size': 120},\n",
       "  'slicing': {'split-large-chunks': None}},\n",
       " 'optimization': {'annotations': {'fuse': True},\n",
       "  'fuse': {'active': None,\n",
       "   'ave-width': 1,\n",
       "   'max-width': None,\n",
       "   'max-height': inf,\n",
       "   'max-depth-new-edges': None,\n",
       "   'subgraphs': None,\n",
       "   'rename-keys': True}},\n",
       " 'admin': {'traceback': {'shorten': ['concurrent[\\\\\\\\\\\\/]futures[\\\\\\\\\\\\/]',\n",
       "    'dask[\\\\\\\\\\\\/](base|core|local|multiprocessing|optimization|threaded|utils)\\\\.py',\n",
       "    'dask[\\\\\\\\\\\\/]array[\\\\\\\\\\\\/]core\\\\.py',\n",
       "    'dask[\\\\\\\\\\\\/]dataframe[\\\\\\\\\\\\/](core|methods)\\\\.py',\n",
       "    'distributed[\\\\\\\\\\\\/](client|scheduler|utils|worker)\\\\.py',\n",
       "    'tornado[\\\\\\\\\\\\/]gen\\\\.py',\n",
       "    'pandas[\\\\\\\\\\\\/]core[\\\\\\\\\\\\/]']}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask.config.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d9c26-ad00-4e0a-b8dc-6fcfb15cfc71",
   "metadata": {},
   "source": [
    "#### start a local Dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97accd08-fd64-47e2-b870-ed24ff71ad9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cluster = LocalCluster(n_workers=4,threads_per_worker=1,processes=False)\n",
    "cluster=LocalCluster(n_workers=28,processes=True,threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c30b0ff-4bea-4b98-9bcf-e6981e29a181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:33811' processes=28 threads=28, memory=0.98 TiB>\n"
     ]
    }
   ],
   "source": [
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503b17d-fda6-4ab3-a465-46398b38e269",
   "metadata": {},
   "source": [
    "# workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c91c04-6783-45e7-b87c-10e27706609a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "    \n",
    "## make masks for ENSO composites\n",
    "\n",
    "### load ONI data\n",
    "ONI_DF = pd.read_csv('/g/data/xv83/users/tm4888/data/ENSO/NCAR_ONI.csv')\n",
    "ONI_DF.set_index('datetime',inplace=True)\n",
    "ONI_DF.index = pd.to_datetime(ONI_DF.index)\n",
    "###\n",
    "el_nino_threshold = 0.5\n",
    "la_nina_threshold = -0.5\n",
    "el_nino_threshold_months = ONI_DF[\"ONI\"].ge(el_nino_threshold)\n",
    "la_nina_threshold_months = ONI_DF[\"ONI\"].le(la_nina_threshold)\n",
    "ONI_DF = pd.concat([ONI_DF, el_nino_threshold_months.rename('El Nino threshold')], axis=1)\n",
    "ONI_DF = pd.concat([ONI_DF, la_nina_threshold_months.rename('La Nina threshold')], axis=1)\n",
    "ONI_DF = pd.concat([ONI_DF, el_nino_threshold_months.diff().ne(0).cumsum().rename('El Nino event group ID')], axis=1)\n",
    "ONI_DF = pd.concat([ONI_DF, la_nina_threshold_months.diff().ne(0).cumsum().rename('La Nina event group ID')], axis=1)\n",
    "#\n",
    "El_Nino_Series = ONI_DF.groupby('El Nino event group ID')['ONI'].filter(lambda x: len(x) >= 5,dropna=False).where(ONI_DF['El Nino threshold'] == True)\n",
    "ONI_DF = pd.concat([ONI_DF, El_Nino_Series.rename('El Nino')], axis=1)\n",
    "La_Nina_Series = ONI_DF.groupby('La Nina event group ID')['ONI'].filter(lambda x: len(x) >= 5,dropna=False).where(ONI_DF['La Nina threshold'] == True)\n",
    "ONI_DF = pd.concat([ONI_DF, La_Nina_Series.rename('La Nina')], axis=1)\n",
    "#\n",
    "ONI_DF_BRANtime = ONI_DF['1993-01':'2023-06']\n",
    "ONI_DF_BRANtime['El Nino LOGICAL'] = ONI_DF_BRANtime['El Nino'].notnull()\n",
    "ONI_DF_BRANtime['La Nina LOGICAL'] = ONI_DF_BRANtime['La Nina'].notnull()\n",
    "# shift back from middle of month\n",
    "ONI_DF_BRANtime.index += pd.Timedelta(-14, 'd')\n",
    "# modify end value for upsample\n",
    "ONI_DF_BRANtime.loc[pd.to_datetime('2023-07-01 00:00:00')] = 'NaN'\n",
    "#upsample\n",
    "ONI_DF_BRANtime = ONI_DF_BRANtime.resample('D').ffill()\n",
    "#drop last dummy date\n",
    "ONI_DF_BRANtime = ONI_DF_BRANtime[:-1]\n",
    "#\n",
    "\n",
    "### run var on what variable\n",
    "#var_name = 'temp'\n",
    "var_name = 'mld'\n",
    "#var_name = 'eta_t'\n",
    "\n",
    "# Write log file\n",
    "log_path = '/scratch/es60/ard/reanalysis/BRAN2020/ARD/logs/'\n",
    "log_file = log_path + f'log_{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")}_{var_name}_stats.txt'\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(f'{var_name} start processing . . .\\n')\n",
    "\n",
    "#\n",
    "zarr_path = '/scratch/es60/ard/reanalysis/BRAN2020/ARD/'\n",
    "path_dict = {'eta_t':'BRAN2020-daily-eta_t-chunk4time-v14032024.zarr',\n",
    "                 'mld':'BRAN2020-daily-mld-chunk4time-v04042024.zarr',\n",
    "                 'temp':'BRAN2020-daily-temp-chunk4time-v07022024.zarr'}\n",
    "results_path = '/g/data/es60/users/thomas_moore/clim_demo_results/daily/draft_delivery/'\n",
    "results_file = 'BRAN2020_clim_demo_'+var_name+'.nc'\n",
    "\n",
    "########### vvvvvv for testing\n",
    "#path_dict = {'eta_t':'error',\n",
    "#                 'mld':'coarsened_tests/BRAN2020_mld_chunked4time_COARSENED.zarr',\n",
    "#                 'temp':'error'}\n",
    "#results_path = '/g/data/es60/users/thomas_moore/clim_demo_results/daily/draft_delivery/COARSENED/'\n",
    "#results_file = 'BRAN2020_clim_demo_'+var_name+'_COARSENED.nc'\n",
    "##### ^^^^^ for testing \n",
    "\n",
    "depth_dict = {'eta_t':None,'mld':None,'temp':'st_ocean'}\n",
    "lon_dict = {'eta_t':'xt_ocean','mld':'xt_ocean','temp':'xt_ocean'}\n",
    "lat_dict = {'eta_t':'yt_ocean','mld':'yt_ocean','temp':'yt_ocean'}\n",
    "time_dim = 'Time'\n",
    "\n",
    "collection_path = zarr_path + path_dict[var_name]\n",
    "# load BRAN data\n",
    "ds = xr.open_zarr(collection_path,consolidated=True)\n",
    "\n",
    "#\n",
    "### ENSO masks\n",
    "El_Nino_mask = ONI_DF_BRANtime['El Nino LOGICAL']\n",
    "El_Nino_mask = El_Nino_mask.to_xarray()\n",
    "El_Nino_mask = El_Nino_mask.rename({'datetime':'Time'})\n",
    "sync_Time = ds.Time\n",
    "El_Nino_mask['Time'] = sync_Time\n",
    "#\n",
    "La_Nina_mask = ONI_DF_BRANtime['La Nina LOGICAL']\n",
    "La_Nina_mask = La_Nina_mask.to_xarray()\n",
    "La_Nina_mask = La_Nina_mask.rename({'datetime':'Time'})\n",
    "sync_Time = ds.Time\n",
    "La_Nina_mask['Time'] = sync_Time\n",
    "#\n",
    "ONI_DF_BRANtime['Neutral LOGICAL'] = (ONI_DF_BRANtime['El Nino LOGICAL'] == False) & (ONI_DF_BRANtime['La Nina LOGICAL'] == False)\n",
    "neutral_mask = ONI_DF_BRANtime['La Nina LOGICAL']\n",
    "neutral_mask = neutral_mask.to_xarray()\n",
    "neutral_mask = neutral_mask.rename({'datetime':'Time'})\n",
    "sync_Time = ds.Time\n",
    "neutral_mask['Time'] = sync_Time\n",
    "\n",
    "### calculate \"all time\" stats\n",
    "\n",
    "clim_ds = xr.merge([ds.groupby(time_dim+'.month').mean(dim=time_dim,skipna=True).rename({var_name:'mean_'+var_name}),\n",
    "                      ds.groupby(time_dim+'.month').min(dim=time_dim,skipna=True).rename({var_name:'min_'+var_name}),\n",
    "                      ds.groupby(time_dim+'.month').max(dim=time_dim,skipna=True).rename({var_name:'max_'+var_name}),\n",
    "                      ds.groupby(time_dim+'.month').std(dim=time_dim,skipna=True).rename({var_name:'std_'+var_name}),\n",
    "                      ds.groupby(time_dim+'.month').median(dim=time_dim,skipna=True).rename({var_name:'median_'+var_name})\n",
    "])\n",
    "quant = ds.groupby(time_dim+'.month').quantile([0.05,0.95],skipna=True,dim=time_dim).astype(np.float32)\n",
    "quant_ds = xr.merge([quant.isel(quantile=0).reset_coords(drop=True).rename({var_name:'quantile_05_'+var_name}),quant.isel(quantile=1).reset_coords(drop=True).rename({var_name:'quantile_95_'+var_name})])\n",
    "result_ds = xr.merge([clim_ds,quant_ds])\n",
    "\n",
    "#### mask out data\n",
    "\n",
    "El_Nino_ds = ds.where(El_Nino_mask)\n",
    "La_Nina_ds = ds.where(La_Nina_mask)\n",
    "neutral_ds = ds.where(neutral_mask)\n",
    "\n",
    "##### El Nino calc\n",
    "clim_El_Nino_ds = xr.merge([El_Nino_ds.groupby(time_dim+'.month').mean(dim=time_dim,skipna=True).rename({var_name:'mean_'+'el_nino_'+var_name}),\n",
    "                      El_Nino_ds.groupby(time_dim+'.month').min(dim=time_dim,skipna=True).rename({var_name:'min_'+'el_nino_'+var_name}),\n",
    "                      El_Nino_ds.groupby(time_dim+'.month').max(dim=time_dim,skipna=True).rename({var_name:'max_'+'el_nino_'+var_name}),\n",
    "                      El_Nino_ds.groupby(time_dim+'.month').std(dim=time_dim,skipna=True).rename({var_name:'std_'+'el_nino_'+var_name}),\n",
    "                      El_Nino_ds.groupby(time_dim+'.month').median(dim=time_dim,skipna=True).rename({var_name:'median_'+'el_nino_'+var_name})\n",
    "])\n",
    "quant_El_Nino = El_Nino_ds.groupby(time_dim+'.month').quantile([0.05,0.95],skipna=True,dim=time_dim).astype(np.float32)\n",
    "quant_El_Nino_ds = xr.merge([quant_El_Nino.isel(quantile=0).reset_coords(drop=True).rename({var_name:'quantile_05_'+'el_nino_'+var_name}),quant_El_Nino.isel(quantile=1).reset_coords(drop=True).rename({var_name:'quantile_95_'+'el_nino_'+var_name})])\n",
    "result_El_Nino_ds = xr.merge([clim_El_Nino_ds,quant_El_Nino_ds])\n",
    "\n",
    "#### La Nina calc\n",
    "clim_La_Nina_ds = xr.merge([La_Nina_ds.groupby(time_dim+'.month').mean(dim=time_dim,skipna=True).rename({var_name:'mean_'+'la_nina_'+var_name}),\n",
    "                      La_Nina_ds.groupby(time_dim+'.month').min(dim=time_dim,skipna=True).rename({var_name:'min_'+'la_nina_'+var_name}),\n",
    "                      La_Nina_ds.groupby(time_dim+'.month').max(dim=time_dim,skipna=True).rename({var_name:'max_'+'la_nina_'+var_name}),\n",
    "                      La_Nina_ds.groupby(time_dim+'.month').std(dim=time_dim,skipna=True).rename({var_name:'std_'+'la_nina_'+var_name}),\n",
    "                      La_Nina_ds.groupby(time_dim+'.month').median(dim=time_dim,skipna=True).rename({var_name:'median_'+'la_nina_'+var_name})\n",
    "])\n",
    "quant_La_Nina = La_Nina_ds.groupby(time_dim+'.month').quantile([0.05,0.95],skipna=True,dim=time_dim).astype(np.float32)\n",
    "quant_La_Nina_ds = xr.merge([quant_La_Nina.isel(quantile=0).reset_coords(drop=True).rename({var_name:'quantile_05_'+'la_nina_'+var_name}),quant_La_Nina.isel(quantile=1).reset_coords(drop=True).rename({var_name:'quantile_95_'+'la_nina_'+var_name})])\n",
    "result_La_Nina_ds = xr.merge([clim_La_Nina_ds,quant_La_Nina_ds])\n",
    "#### neutral calc\n",
    "clim_neutral_ds = xr.merge([neutral_ds.groupby(time_dim+'.month').mean(dim=time_dim,skipna=True).rename({var_name:'mean_'+'neutral_'+var_name}),\n",
    "                      neutral_ds.groupby(time_dim+'.month').min(dim=time_dim,skipna=True).rename({var_name:'min_'+'neutral_'+var_name}),\n",
    "                      neutral_ds.groupby(time_dim+'.month').max(dim=time_dim,skipna=True).rename({var_name:'max_'+'neutral_'+var_name}),\n",
    "                      neutral_ds.groupby(time_dim+'.month').std(dim=time_dim,skipna=True).rename({var_name:'std_'+'neutral_'+var_name}),\n",
    "                      neutral_ds.groupby(time_dim+'.month').median(dim=time_dim,skipna=True).rename({var_name:'median_'+'neutral_'+var_name})\n",
    "])\n",
    "quant_neutral = neutral_ds.groupby(time_dim+'.month').quantile([0.05,0.95],skipna=True,dim=time_dim).astype(np.float32)\n",
    "quant_neutral_ds = xr.merge([quant_neutral.isel(quantile=0).reset_coords(drop=True).rename({var_name:'quantile_05_'+'neutral_'+var_name}),quant_neutral.isel(quantile=1).reset_coords(drop=True).rename({var_name:'quantile_95_'+'neutral_'+var_name})])\n",
    "result_neutral_ds = xr.merge([clim_neutral_ds,quant_neutral_ds])\n",
    "#\n",
    "result_ds = xr.merge([result_ds,result_El_Nino_ds,result_La_Nina_ds,result_neutral_ds])\n",
    "\n",
    "result_ds.to_netcdf(results_path+results_file,engine='netcdf4')\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(f'{var_name} .... finished processing\\n DONE!\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9d5ae-fe32-45d2-9af6-6d13ea3ac988",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# $The$ $End$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920d781-f954-436b-94a8-f58d899f5150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fd9c2-7539-4b55-b7b0-4f05c78bba27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
